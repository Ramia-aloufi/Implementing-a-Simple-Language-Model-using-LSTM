{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Re4_ymvKGUP",
        "outputId": "2c6921a5-db12-493f-8498-b32a234ef4d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [50/200], Loss: 0.0003\n",
            "Epoch [100/200], Loss: 0.0001\n",
            "Epoch [150/200], Loss: 0.0001\n",
            "Epoch [200/200], Loss: 0.0001\n",
            "\n",
            "Seed: 'natural language processing is'\n",
            "Generated: 'natural language processing is a field of artificial intelligence. it focuses on the interaction between computers and humans in natural language. the goal of'\n",
            "\n",
            "Seed: 'the goal of nlp'\n",
            "Generated: 'the goal of nlp is to enable computers to understand, interpret, and generate human language. rnns are commonly used in nlp for tasks like'\n",
            "\n",
            "Seed: 'computers and humans'\n",
            "Generated: 'computers and humans in natural language. the goal of nlp is to enable computers to understand, interpret, and generate human language. rnns are'\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# Sample text for language modeling\n",
        "text = \"\"\"\n",
        "Natural language processing is a field of artificial intelligence.\n",
        "It focuses on the interaction between computers and humans in natural language.\n",
        "The goal of NLP is to enable computers to understand, interpret, and generate human language.\n",
        "RNNs are commonly used in NLP for tasks like language modeling and sentiment analysis.\n",
        "\"\"\"\n",
        "\n",
        "# Preprocess the text\n",
        "words = text.lower().replace('\\n', ' ').split()\n",
        "vocab = sorted(set(words))\n",
        "word_to_idx = {word: i for i, word in enumerate(vocab)}\n",
        "idx_to_word = {i: word for i, word in enumerate(vocab)}\n",
        "\n",
        "# Create input-output pairs for next word prediction\n",
        "# Input: sequence of words, Output: next word\n",
        "sequence_length = 3  # Number of previous words to use\n",
        "input_sequences = []\n",
        "output_words = []\n",
        "\n",
        "for i in range(len(words) - sequence_length):\n",
        "    input_sequences.append([word_to_idx[words[j]] for j in range(i, i + sequence_length)])\n",
        "    output_words.append(word_to_idx[words[i + sequence_length]])\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X = torch.tensor(input_sequences, dtype=torch.long)\n",
        "y = torch.tensor(output_words, dtype=torch.long)\n",
        "\n",
        "# Define a language model using RNN\n",
        "class LanguageModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
        "        super(LanguageModel, self).__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, sequence):\n",
        "        # sequence shape: [batch size, seq length]\n",
        "\n",
        "        embedded = self.embedding(sequence)\n",
        "        # embedded shape: [batch size, seq length, embedding dim]\n",
        "\n",
        "        output, (hidden, cell) = self.rnn(embedded)\n",
        "        # output shape: [batch size, seq length, hidden dim]\n",
        "\n",
        "        # Use the output from the last time step\n",
        "        output = output[:, -1, :]\n",
        "        # output shape: [batch size, hidden dim]\n",
        "\n",
        "        prediction = self.fc(output)\n",
        "        # prediction shape: [batch size, vocab size]\n",
        "\n",
        "        return prediction\n",
        "\n",
        "# Set up the model\n",
        "vocab_size = len(vocab)\n",
        "embedding_dim = 50\n",
        "hidden_dim = 100\n",
        "\n",
        "model = LanguageModel(vocab_size, embedding_dim, hidden_dim)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# Train the model\n",
        "num_epochs = 200\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Forward pass\n",
        "    outputs = model(X)\n",
        "\n",
        "    # Compute loss\n",
        "    loss = criterion(outputs, y)\n",
        "\n",
        "    # Backward pass and optimize\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Print progress\n",
        "    if (epoch + 1) % 50 == 0:\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Generate text using the trained model\n",
        "def generate_text(model, seed_text, word_to_idx, idx_to_word, max_length=20):\n",
        "    model.eval()\n",
        "    words = seed_text.lower().split()\n",
        "\n",
        "    # Use the last 'sequence_length' words as our initial sequence\n",
        "    current_sequence = [word_to_idx.get(word, random.randint(0, len(vocab)-1)) for word in words[-sequence_length:]]\n",
        "\n",
        "    generated_words = words.copy()\n",
        "\n",
        "    for _ in range(max_length):\n",
        "        # Convert sequence to tensor\n",
        "        sequence_tensor = torch.tensor([current_sequence], dtype=torch.long)\n",
        "\n",
        "        # Get prediction\n",
        "        with torch.no_grad():\n",
        "            output = model(sequence_tensor)\n",
        "\n",
        "        # Get the word with highest probability\n",
        "        _, predicted_idx = torch.max(output, dim=1)\n",
        "        predicted_word = idx_to_word[predicted_idx.item()]\n",
        "\n",
        "        # Add the predicted word to our generated words\n",
        "        generated_words.append(predicted_word)\n",
        "\n",
        "        # Update the sequence for next prediction (remove first element, add predicted)\n",
        "        current_sequence = current_sequence[1:] + [predicted_idx.item()]\n",
        "\n",
        "    return ' '.join(generated_words)\n",
        "\n",
        "# Test the language model with different seed texts\n",
        "seed_texts = [\n",
        "    \"natural language processing is\",\n",
        "    \"the goal of nlp\",\n",
        "    \"computers and humans\"\n",
        "]\n",
        "\n",
        "for seed in seed_texts:\n",
        "    generated = generate_text(model, seed, word_to_idx, idx_to_word)\n",
        "    print(f\"\\nSeed: '{seed}'\")\n",
        "    print(f\"Generated: '{generated}'\")"
      ]
    }
  ]
}